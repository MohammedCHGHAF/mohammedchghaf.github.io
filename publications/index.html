<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
      new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
      j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
      'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
      })(window,document,'script','dataLayer','GTM-T8Q42WZN');</script> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Publications | Mohammed CHGHAF </title> <meta name="author" content="Mohammed CHGHAF"> <meta name="description" content="This is the personal website of Mohammed Chghaf, PhD. "> <meta name="keywords" content="Chghaf, Mohammed, SLAM, Robotics, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/1mc2.png?5a80c40d01eaa6770baeab1debd0cc52"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mohammedchghaf.github.io/publications/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T8Q42WZN" height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Mohammed <span class="font-weight-bold">CHGHAF</span> </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">Publications <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/talks/">Talks </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Publications</h1> <p class="post-description"></p> </header> <article> <p>An up-to-date list is available on <a href="https://scholar.google.com/citations?user=rSdm4WkAAAAJ&amp;hl" rel="external nofollow noopener" target="_blank">Google Scholar</a>.</p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MDPI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mdpi_2025-480.webp 480w,/assets/img/publication_preview/mdpi_2025-800.webp 800w,/assets/img/publication_preview/mdpi_2025-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/mdpi_2025.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mdpi_2025.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chghaf:hal-03647769" class="col-sm-8"> <div class="title">Extended Study of a Multi-Modal Loop Closure Detection Framework for SLAM Applications</div> <div class="author"> <em>Mohammed Chghaf</em>, <a href="https://scholar.google.com/citations?user=AOD98yEAAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Sergio Rodriguez</a>, and <a href="https://scholar.google.com/citations?user=XeCMgjwAAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Abdelhafid El Ouardi</a> </div> <div class="periodical"> <em>Electronics</em>, Jan 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3390/electronics14030421" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.mdpi.com/2079-9292/14/3/421/pdf?version=1737539131" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.mdpi.com/2079-9292/14/3/421" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.3390/electronics14030421" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=rSdm4WkAAAAJ&amp;citation_for_view=rSdm4WkAAAAJ:WF5omc3nYNoC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Loop Closure (LC) is a crucial task in Simultaneous Localization and Mapping (SLAM) for Autonomous Ground Vehicles (AGV). It is an active research area because it improves global localization efficiency. The consistency of the global map and the accuracy of the AGV’s location in an unknown environment are highly correlated with the efficiency and robustness of Loop Closure Detection (LCD), especially when facing environmental changes or data unavailability. We propose to introduce multimodal complementary data to increase the algorithms’ resilience. Various methods using different data sources have been proposed to achieve precise place recognition. However, integrating a multimodal loop-closure fusion process that combines multiple information sources within a SLAM system has been explored less. Additionally, existing multimodal place recognition techniques are often difficult to integrate into existing frameworks. In this paper, we propose a fusion scheme of multiple place recognition methods based on camera and LiDAR data for a robust multimodal LCD. The presented approach uses Similarity-Guided Particle Filtering (SGPF) to identify and verify candidates for loop closure. Based on the ORB-SLAM2 framework, the proposed method uses two perception sensors (camera and LiDAR) under two data representation models for each. Our experiments on both KITTI and a self-collected dataset show that our approach outperforms the state-of-the-art methods in terms of place recognition metrics or localization accuracy metrics. The proposed Multi-Modal Loop Closure (MMLC) framework enhances the robustness and accuracy of AGV’s localization by fusing multiple sensor modalities, ensuring consistent performance across diverse environments. Its real-time operation and early loop closure detection enable timely trajectory corrections, reducing navigation errors and supporting cost-effective deployment with adaptable sensor configurations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chghaf:hal-03647769</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Extended Study of a Multi-Modal Loop Closure Detection Framework for SLAM Applications}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chghaf, Mohammed and Rodriguez, Sergio and El Ouardi, Abdelhafid}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.mdpi.com/2079-9292/14/3/421}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{Electronics}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{MDPI}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{421}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3390/electronics14030421}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">JINT</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/jint_2023-480.webp 480w,/assets/img/publication_preview/jint_2023-800.webp 800w,/assets/img/publication_preview/jint_2023-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/jint_2023.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="jint_2023.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chghaf:hal-03647770" class="col-sm-8"> <div class="title">An Extended HOOFR SLAM Algorithm Using IR-D Sensor Data for Outdoor Autonomous Vehicle Localization</div> <div class="author"> <a href="https://scholar.google.com/citations?user=zWqnN-AAAAAJ&amp;hl=fr&amp;authuser=1&amp;oi=ao" rel="external nofollow noopener" target="_blank">Imad El Bouazzaoui</a>, <em>Mohammed Chghaf</em>, <a href="https://scholar.google.com/citations?user=AOD98yEAAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Sergio Rodriguez</a>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Dai Duong Nguyen, Abdelhafid El Ouardi' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Journal of Intelligent and Robotic Systems</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/s10846-023-01975-3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://link.springer.com/article/10.1007/s10846-023-01975-3" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://link.springer.com/article/10.1007/s10846-022-01582-8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Several works have been carried out in the realm of RGB-D SLAM development, yet they have neither been thoroughly assessed nor adapted for outdoor vehicular contexts. This paper proposes an extension of HOOFR SLAM to an enhanced IR-D modality applied to an autonomous vehicle in an outdoor environment. We address the most prevalent camera issues in outdoor contexts: environments with an image-dominant overcast sky and the presence of dynamic objects. We used a depth-based filtering method to identify outlier points based on their depth value. The method is robust against outliers and also computationally inexpensive. For faster processing, we suggest optimization of the pose estimation block by replacing the RANSAC method used for essential matrix estimation with PROSAC. We assessed the algorithm using a self-collected IR-D dataset gathered by the SATIE laboratory instrumented vehicle using a PC and an embedded architecture. We compared the measurement results to those of the most advanced algorithms by assessing translational error and average processing time. The results revealed a significant reduction in localization errors and a significant gain in processing speed compared to the state-of-the-art stereo (HOOFR SLAM) and RGB-D algorithms (Orb-slam2, Rtab-map).</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chghaf:hal-03647770</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{An Extended HOOFR SLAM Algorithm Using IR-D Sensor Data for Outdoor Autonomous Vehicle Localization}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{El Bouazzaoui, Imad and Chghaf, Mohammed and Rodriguez, Sergio and Nguyen, Dai Duong and El Ouardi, Abdelhafid}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://link.springer.com/article/10.1007/s10846-023-01975-3}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{Journal of Intelligent and Robotic Systems}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{Springer}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{109}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{56}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">oct</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s10846-023-01975-3}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{false}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">THESIS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/thesis_2023-480.webp 480w,/assets/img/publication_preview/thesis_2023-800.webp 800w,/assets/img/publication_preview/thesis_2023-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/thesis_2023.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="thesis_2023.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chghaf2023_thesis" class="col-sm-8"> <div class="title">Towards a Multimodal Loop Closure System for Real-Time Embedded SLAM Applications</div> <div class="author"> <em>Mohammed Chghaf</em> </div> <div class="periodical"> <em></em> Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://theses.hal.science/tel-04756884/document" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://theses.fr/2023UPAST133" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chghaf2023_thesis</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Towards a Multimodal Loop Closure System for Real-Time Embedded SLAM Applications}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chghaf, Mohammed}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0921889023000854}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{SLAM, Camera, LiDAR, Localization, Loop closure, Mapping, Fusion, Multimodal}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{false}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RAS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmlc_2023-480.webp 480w,/assets/img/publication_preview/mmlc_2023-800.webp 800w,/assets/img/publication_preview/mmlc_2023-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/mmlc_2023.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmlc_2023.jpg" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chghaf2023_ras" class="col-sm-8"> <div class="title">A Multimodal Loop Closure Fusion for Autonomous Vehicles SLAM</div> <div class="author"> <em>Mohammed Chghaf</em>, <a href="https://scholar.google.com/citations?user=AOD98yEAAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Sergio Rodriguez</a>, and <a href="https://scholar.google.com/citations?user=XeCMgjwAAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Abdelhafid El Ouardi</a> </div> <div class="periodical"> <em>Robotics and Autonomous Systems</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.robot.2023.104446" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://authors.elsevier.com/a/1h0wl3HdG3lG4M" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0921889023000854" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1016/j.robot.2023.104446" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=rSdm4WkAAAAJ&amp;citation_for_view=rSdm4WkAAAAJ:zYLM7Y9cAGgC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-5-4285F4?logo=googlescholar&amp;labelColor=beige" alt="5 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Place recognition and loop closure detection are critical steps in the process of Simultaneous Localization and Mapping (SLAM). Indeed, the ability to determine whether an Autonomous Ground Vehicle (AGV) has returned to a previously visited place is highly important in the context of building a reliable SLAM system. In order to build a consistent global map and to localize the AGV with high confidence in an unknown environment, it is crucial to reduce the cumulative error generated by pose estimation. Although multiple approaches using various data sources have been proposed in order to provide an accurate pose estimation, fewer studies have focused on the integration of a multimodal process to detect loop closure. In this work, we present a novel approach to leverage multiple modalities for a robust and reliable loop closure detection. Our method is based on Similarity-Guided Particle Filtering (SGPF) for the search and validation of Loop Closure Candidates (LCCs). We validate the proposed Multimodal Loop Closure (MMLC) by using two perception modalities based on Bag-of-Words and Scan Context techniques for camera-based and LiDAR-based place recognition, respectively. The efficiency of our method has been evaluated on both KITTI and a self-collected dataset. Compared to the classical loop closure used in ORB-SLAM2, the suggested approach reduces the Absolute Trajectory Error (ATE) by up to 54% and the cumulative error during run-time by up to 62.63%. Finally, 100% of the loops are accurately detected and the ground truth distance between the current pose and the LC is less than 3m in 98% of the cases.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chghaf2023_ras</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{A Multimodal Loop Closure Fusion for Autonomous Vehicles SLAM}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chghaf, Mohammed and Rodriguez, Sergio and El Ouardi, Abdelhafid}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0921889023000854}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{Robotics and Autonomous Systems}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{Elsevier}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{165}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{104446}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0921-8890}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.robot.2023.104446}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{SLAM, Camera, LiDAR, Localization, Loop closure, Mapping, Fusion, Multimodal}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SPIE+OO</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/spie_2023-480.webp 480w,/assets/img/publication_preview/spie_2023-800.webp 800w,/assets/img/publication_preview/spie_2023-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/spie_2023.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="spie_2023.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chghaf2023_spie" class="col-sm-8"> <div class="title">Real-time large-scale place recognition for autonomous ground vehicles using a spatial descriptor</div> <div class="author"> <em>Mohammed Chghaf</em>, <a href="https://scholar.google.com/citations?user=AOD98yEAAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Sergio Rodriguez</a>, and <a href="https://scholar.google.com/citations?user=XeCMgjwAAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Abdelhafid El Ouardi</a> </div> <div class="periodical"> <em>In SPIE Optics and Optoelectronics</em>, Oct 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://spie.org/optics-optoelectronics/presentation/Real-time-large-scale-place-recognition-for-autonomous-ground-vehicles/12571-2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://spie.org/optics-optoelectronics/presentation/Real-time-large-scale-place-recognition-for-autonomous-ground-vehicles/12571-2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Place recognition is a key task in an autonomous vehicle’s Simultaneous Localization and Mapping (SLAM). The motion estimation is bound to drift over time due to cumulative errors. Fortunately, the correct identification of a revisited area provided by the place recognition module enables further optimizations that correct drifting errors if detected in real-time. Place recognition based on structural information of the scene is more robust to luminosity changes that can lead to false detections in the case of feature-based descriptors. However, they were mainly investigated in the context of depth sensors. Inspired by a LiDAR-based descriptor, we present a global geometric descriptor based on the structural information captured by a stereo camera. Using this descriptor, we can achieve real-time place recognition by focusing on the structural appearance of the scene derived from a 3D vision system. First, we introduce the approach used to record the 3D structural information of the visible space based on stereo images. Then, we conduct a parametric optimization protocol for precise place recognition in a given environment. Our experiments on the KITTI dataset show that the proposed approach is comparable to state-of-the-art methods, all while being low-cost. We studied the algorithm’s complexity to propose an optimized parallelization on GPU and SoC architectures. Performance evaluation on different hardware (GeForce RTX 3080, Jetson AGX Xavier, and Arria 10 SoC) shows that the real-time requirements of an embedded system are met. Compared to a CPU implementation, processing times showed a speed-up between 7x and 30x, depending on the architecture.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chghaf2023_spie</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Real-time large-scale place recognition for autonomous ground vehicles using a spatial descriptor}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chghaf, Mohammed and Rodriguez, Sergio and El Ouardi, Abdelhafid}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{SPIE Optics and Optoelectronics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">JINT</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/survey_2022-480.webp 480w,/assets/img/publication_preview/survey_2022-800.webp 800w,/assets/img/publication_preview/survey_2022-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/survey_2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="survey_2022.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chghaf:hal-03647771" class="col-sm-8"> <div class="title">Camera, LiDAR and Multi-modal SLAM Systems for Autonomous Ground Vehicles: a Survey</div> <div class="author"> <em>Mohammed Chghaf</em>, <a href="https://scholar.google.com/citations?user=AOD98yEAAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Sergio Rodriguez</a>, and <a href="https://scholar.google.com/citations?user=XeCMgjwAAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Abdelhafid El Ouardi</a> </div> <div class="periodical"> <em>Journal of Intelligent and Robotic Systems</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/s10846-022-01582-8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://rdcu.be/cLMWA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://link.springer.com/article/10.1007/s10846-022-01582-8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1007/s10846-022-01582-8" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=rSdm4WkAAAAJ&amp;citation_for_view=rSdm4WkAAAAJ:qjMakFHDy7sC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-41-4285F4?logo=googlescholar&amp;labelColor=beige" alt="41 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Simultaneous Localization and Mapping (SLAM) have been widely studied over the last years for autonomous vehicles. SLAM achieves its purpose by constructing a map of the unknown environment while keeping track of the location. A major challenge, which is paramount during the design of SLAM systems, lies in the efficient use of onboard sensors to perceive the environment. The most widely applied algorithms are camera-based SLAM and LiDAR-based SLAM. Recent research focuses on the fusion of camera-based and LiDAR-based frameworks that show promising results. In this paper, we present a study of commonly used sensors and the fundamental theories behind SLAM algorithms. The study then presents the hardware architectures used to process these algorithms and the performance obtained when possible. Secondly, we highlight state-of-the-art methodologies in each modality and in the multi-modal framework. A brief comparison followed by future challenges is then underlined. Additionally, we provide insights to possible fusion approaches that can increase the robustness and accuracy of modern SLAM algorithms; hence allowing the hardware-software co-design of embedded systems taking into account the algorithmic complexity and the embedded architectures and real-time constraints.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chghaf:hal-03647771</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Camera, LiDAR and Multi-modal SLAM Systems for Autonomous Ground Vehicles: a Survey}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chghaf, Mohammed and Rodriguez, Sergio and El Ouardi, Abdelhafid}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://link.springer.com/article/10.1007/s10846-022-01582-8?noAccess=true}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{Journal of Intelligent and Robotic Systems}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{Springer Verlag}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{105}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s10846-022-01582-8}</span><span class="p">,</span>
  <span class="na">hal_id</span> <span class="p">=</span> <span class="s">{hal-03647769}</span><span class="p">,</span>
  <span class="na">hal_version</span> <span class="p">=</span> <span class="s">{v1}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">GRETSI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/gretsi_2022-480.webp 480w,/assets/img/publication_preview/gretsi_2022-800.webp 800w,/assets/img/publication_preview/gretsi_2022-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/gretsi_2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gretsi_2022.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chghaf2022stereo" class="col-sm-8"> <div class="title">A stereo vision geometric descriptor for place recognition and its GPU acceleration for autonomous vehicles applications</div> <div class="author"> <em>Mohammed Chghaf</em>, <a href="https://scholar.google.com/citations?user=AOD98yEAAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Sergio Rodriguez</a>, and <a href="https://scholar.google.com/citations?user=XeCMgjwAAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Abdelhafid El Ouardi</a> </div> <div class="periodical"> <em>In XXVIIIème Colloque Francophone de Traitement du Signal et des Images, GRETSI’22</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://gretsi.fr/data/colloque/pdf/2022_chghaf1024.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="http://gretsi.fr/colloque2022/programme/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>In this paper we present a geometric global descriptor dedicated for place recognition in autonomous vehicles applications that is based on stereo camera generated point clouds. We first present the approach used to record the 3D structure of the visible space. Then, we propose a parametric optimization to achieve the best performance by coupling the dedicated sensor (Stereo Camera) and the used algorithm. Finally, we propose a GPU implementation based on CUDA. Compared to a CPU, processing times on GPU are accelerated 7 times on a Jetson AGX Xavier and 30 times using a GeForce RTX 3080.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chghaf2022stereo</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A stereo vision geometric descriptor for place recognition and its GPU acceleration for autonomous vehicles applications}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chghaf, Mohammed and Rodriguez, Sergio and El Ouardi, Abdelhafid}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{XXVIII{\`e}me Colloque Francophone de Traitement du Signal et des Images, GRETSI'22}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2021</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">GdR-Rob</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/2rm_2021-480.webp 480w,/assets/img/publication_preview/2rm_2021-800.webp 800w,/assets/img/publication_preview/2rm_2021-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/2rm_2021.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="2rm_2021.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="chghaf2021" class="col-sm-8"> <div class="title">SLAM pour véhicules terrestres: évaluation des méthodes de l’état de l’art sur un véhicule instrumenté</div> <div class="author"> <em>Mohammed Chghaf</em>, <a href="https://scholar.google.com/citations?user=AOD98yEAAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Sergio Rodriguez</a>, and <a href="https://scholar.google.com/citations?user=XeCMgjwAAAAJ&amp;hl=fr" rel="external nofollow noopener" target="_blank">Abdelhafid El Ouardi</a> </div> <div class="periodical"> <em>In Robotique Mobile</em>, Apr 2021 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://wiki.2rm.cnrs.fr/JT_robmob_2021?action=AttachFile&amp;do=view&amp;target=Pr%C3%A9sentation_MC_GDR_ROB_2RM_2021.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://peertube.laas.fr/w/4e241410-b16d-4cbd-bf14-3c113e6f5ec9" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="abstract hidden"> <p>Actuellement, nous assistons à une évolution rapide des besoins en termes de fonctions de localisation et de cartographie simultanées (dites SLAM) dans de nombreux domaines applicatifs et principalement pour les véhicules autonomes. La recherche dans ce domaine a conduit à plusieurs avancées pour le traitement de données riches en indicateurs et en sémantiques d’informations issues de plusieurs capteurs extéroceptifs (ex. vision RGB-D, LiDAR,...). Cependant, peu de travaux se sont intéressés aux exigences de calcul élevées pour le traitement temps-réel et embarqué ainsi qu’aux contraintes d’intégrité des différentes modalités de perception. Dans cette présentation, nous allons exposer l’état de l’art de la thématique et une étude expérimentale d’algorithmes de localisation et de cartographie simultanées basés sur une seule modalité de perception (vision ou LiDAR). Cette étude fera usage d’un jeu de données recueilli par le véhicule expérimental du laboratoire SATIE. Nous présenterons par la suite des perspectives vers la fusion de ces deux modalités de perception. Finalement, nous aborderons la question de l’embarquabilité, de ses stratégies et les défis qu’elle présente. .</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2020</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RTCSA</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rtcsa_2020-480.webp 480w,/assets/img/publication_preview/rtcsa_2020-800.webp 800w,/assets/img/publication_preview/rtcsa_2020-1400.webp 1400w," type="image/webp" sizes="200px"></source> <img src="/assets/img/publication_preview/rtcsa_2020.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="rtcsa_2020.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="Chghaf2020" class="col-sm-8"> <div class="title">Data distribution on a multi-GPU node for TomoBayes CT reconstruction</div> <div class="author"> <em>Mohammed Chghaf</em>, and Nicolas Gac </div> <div class="periodical"> <em>In The 26th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications</em>, Aug 2020 </div> <div class="periodical"> Virtual conference (Covid) </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://hal.archives-ouvertes.fr/hal-02586239v4/file/main.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.youtube.com/watch?v=gzkpGD-KBpo" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> <a href="https://rtcsa2020.github.io/index/?page=program.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Computed tomography (CT) is an imaging tech- nique that uses iterative algorithms to reconstruct the interior of large volumes. Graphics Processing Units (GPUs) are cur- rently the preferred technology for computation acceleration. However, the collected data storage can exceed the internal memory of current GPUs and therefore requires costly CPU GPU data transfers. In this paper, we present a strategy of data distribution over several GPUs avoiding this bottleneck. We provide experimental results showing that our memory-saving method accelerates the iterative reconstruction. We achieve a better parallelisation efficiency using 8 GPUs to reconstruct a volume of size 4 GB than reconstructions based on centralised data storage on CPU.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">Chghaf2020</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Data distribution on a multi-GPU node for TomoBayes CT reconstruction}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chghaf, Mohammed and Gac, Nicolas}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://hal.archives-ouvertes.fr/hal-02586239}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Virtual conference (Covid)}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{{The 26th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications}}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{South Korea}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">aug</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Multi GPU ; Iterative reconstruction ; Data Distribution}</span><span class="p">,</span>
  <span class="na">hal_id</span> <span class="p">=</span> <span class="s">{hal-02586239}</span><span class="p">,</span>
  <span class="na">hal_version</span> <span class="p">=</span> <span class="s">{v4}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Mohammed CHGHAF. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. Last updated: March 13, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id="></script> <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', '');
  </script> <script defer src="/assets/js/google-analytics-setup.js?12374742c4b1801ba82226e617af7e2d"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>