<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Mohammed CHGHAF </title> <meta name="author" content="Mohammed CHGHAF"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://mohammedchghaf.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Mohammed CHGHAF</span> </h1> <p class="desc">PhD. in Robotics. Localization &amp; HD Mapping @<a href="https://www.ampere.cars/" style="font-weight:bold;color:#0011FF" rel="external nofollow noopener" target="_blank">AMPERE</a>.</p> </header> <article> <div class="profile float-left"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/profile-480.webp 480w,/assets/img/profile-800.webp 800w,/assets/img/profile-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/profile.jpg?d6c6b014e005421512f3e93442b01e7e" class="img-fluid z-depth-1 rounded-circle" width="100%" height="auto" alt="profile.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"></div> </div> <div class="clearfix"> <p>I am a Localization &amp; HD Mapping Engineer specializing in Automated Driving and Advanced Driver Assistance Systems (AD/ADAS) at <a href="https://www.ampere.cars/" rel="external nofollow noopener" target="_blank">AMPERE</a>.</p> <p>I completed my PhD in 2023 from <a href="https://www.universite-paris-saclay.fr/" rel="external nofollow noopener" target="_blank">Paris-Saclay University</a>, where I worked at <a href="https://satie.ens-paris-saclay.fr/fr" rel="external nofollow noopener" target="_blank">SATIE Laboratory</a> under the supervision of <a href="https://hebergement.universite-paris-saclay.fr/sergio.rodriguez/" rel="external nofollow noopener" target="_blank">Sergio Rodríguez Flórez</a> and <a href="https://scholar.google.fr/citations?user=XeCMgjwAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank">Abdelhafid El Ouardi</a>. My doctoral thesis investigated data fusion techniques for multimodal SLAM on embedded systems, enabling efficient real-time localization and mapping for automated driving.</p> <p>Prior to my PhD, I graduated from <a href="https://www.universite-paris-saclay.fr/" rel="external nofollow noopener" target="_blank">Paris-Saclay University</a> (<a href="https://www.universite-paris-saclay.fr/formation/master/electronique-energie-electrique-automatique/m2-systemes-embarques-et-traitement-de-linformation" rel="external nofollow noopener" target="_blank">M2 Embedded Systems &amp; Data Fusion</a>) and <a href="https://www.emi.ac.ma/emi/" rel="external nofollow noopener" target="_blank">Mohammadia School of Engineers</a> (Automation &amp; Control Engineering).</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Jan 18, 2025</th> <td> Our paper <strong>Extended Study of a Multi-Modal Loop Closure Detection Framework for SLAM Applications</strong> is published! you can read it <strong><a href="https://www.mdpi.com/2079-9292/14/3/421" rel="external nofollow noopener" target="_blank">here</a></strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Jan 15, 2024</th> <td> I joined <strong><a href="https://www.ampere.cars/" rel="external nofollow noopener" target="_blank">AMPERE</a></strong> as a Localization &amp; HD Mapping Specialist! </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 31, 2023</th> <td> We published an article on <strong>An Extended HOOFR SLAM Algorithm Using IR-D Sensor Data for Outdoor Autonomous Vehicle Localization</strong>! You can read more <strong><a href="https://link.springer.com/article/10.1007/s10846-023-01975-3" rel="external nofollow noopener" target="_blank">here</a></strong>. </td> </tr> <tr> <th scope="row" style="width: 20%">Oct 19, 2023</th> <td> I defended my PhD thesis! Read more about it <strong><a href="https://theses.fr/2023UPAST133" rel="external nofollow noopener" target="_blank">here</a></strong>! </td> </tr> <tr> <th scope="row" style="width: 20%">May 04, 2023</th> <td> We published an article on <strong>Multimodal Loop Closure Fusion for SLAM in the context of Autonomous Ground Vehicles</strong>! You can read more <strong><a href="https://www.sciencedirect.com/science/article/pii/S0921889023000854" rel="external nofollow noopener" target="_blank">here</a></strong>. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected publications</a> </h2> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">MDPI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mdpi_2025-480.webp 480w,/assets/img/publication_preview/mdpi_2025-800.webp 800w,/assets/img/publication_preview/mdpi_2025-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/mdpi_2025.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mdpi_2025.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="chghaf:hal-03647769" class="col-sm-8"> <div class="title">Extended Study of a Multi-Modal Loop Closure Detection Framework for SLAM Applications</div> <div class="author"> Mohammed Chghaf, Sergio Rodriguez, and Abdelhafid El Ouardi </div> <div class="periodical"> <em>Electronics</em>, Jan 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.3390/electronics14030421" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://www.mdpi.com/2079-9292/14/3/421/pdf?version=1737539131" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.mdpi.com/2079-9292/14/3/421" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Loop Closure (LC) is a crucial task in Simultaneous Localization and Mapping (SLAM) for Autonomous Ground Vehicles (AGV). It is an active research area because it improves global localization efficiency. The consistency of the global map and the accuracy of the AGV’s location in an unknown environment are highly correlated with the efficiency and robustness of Loop Closure Detection (LCD), especially when facing environmental changes or data unavailability. We propose to introduce multimodal complementary data to increase the algorithms’ resilience. Various methods using different data sources have been proposed to achieve precise place recognition. However, integrating a multimodal loop-closure fusion process that combines multiple information sources within a SLAM system has been explored less. Additionally, existing multimodal place recognition techniques are often difficult to integrate into existing frameworks. In this paper, we propose a fusion scheme of multiple place recognition methods based on camera and LiDAR data for a robust multimodal LCD. The presented approach uses Similarity-Guided Particle Filtering (SGPF) to identify and verify candidates for loop closure. Based on the ORB-SLAM2 framework, the proposed method uses two perception sensors (camera and LiDAR) under two data representation models for each. Our experiments on both KITTI and a self-collected dataset show that our approach outperforms the state-of-the-art methods in terms of place recognition metrics or localization accuracy metrics. The proposed Multi-Modal Loop Closure (MMLC) framework enhances the robustness and accuracy of AGV’s localization by fusing multiple sensor modalities, ensuring consistent performance across diverse environments. Its real-time operation and early loop closure detection enable timely trajectory corrections, reducing navigation errors and supporting cost-effective deployment with adaptable sensor configurations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chghaf:hal-03647769</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Extended Study of a Multi-Modal Loop Closure Detection Framework for SLAM Applications}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chghaf, Mohammed and Rodriguez, Sergio and El Ouardi, Abdelhafid}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.mdpi.com/2079-9292/14/3/421}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{Electronics}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{MDPI}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{14}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{3}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{421}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jan</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.3390/electronics14030421}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{false}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">RAS</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmlc_2023-480.webp 480w,/assets/img/publication_preview/mmlc_2023-800.webp 800w,/assets/img/publication_preview/mmlc_2023-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/mmlc_2023.jpg" class="preview z-depth-1 rounded" width="100%" height="auto" alt="mmlc_2023.jpg" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="chghaf2023_ras" class="col-sm-8"> <div class="title">A Multimodal Loop Closure Fusion for Autonomous Vehicles SLAM</div> <div class="author"> Mohammed Chghaf, Sergio Alberto Rodríguez Flórez, and Abdelhafid El Ouardi </div> <div class="periodical"> <em>Robotics and Autonomous Systems</em>, Jan 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1016/j.robot.2023.104446" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://authors.elsevier.com/a/1h0wl3HdG3lG4M" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0921889023000854" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Place recognition and loop closure detection are critical steps in the process of Simultaneous Localization and Mapping (SLAM). Indeed, the ability to determine whether an Autonomous Ground Vehicle (AGV) has returned to a previously visited place is highly important in the context of building a reliable SLAM system. In order to build a consistent global map and to localize the AGV with high confidence in an unknown environment, it is crucial to reduce the cumulative error generated by pose estimation. Although multiple approaches using various data sources have been proposed in order to provide an accurate pose estimation, fewer studies have focused on the integration of a multimodal process to detect loop closure. In this work, we present a novel approach to leverage multiple modalities for a robust and reliable loop closure detection. Our method is based on Similarity-Guided Particle Filtering (SGPF) for the search and validation of Loop Closure Candidates (LCCs). We validate the proposed Multimodal Loop Closure (MMLC) by using two perception modalities based on Bag-of-Words and Scan Context techniques for camera-based and LiDAR-based place recognition, respectively. The efficiency of our method has been evaluated on both KITTI and a self-collected dataset. Compared to the classical loop closure used in ORB-SLAM2, the suggested approach reduces the Absolute Trajectory Error (ATE) by up to 54% and the cumulative error during run-time by up to 62.63%. Finally, 100% of the loops are accurately detected and the ground truth distance between the current pose and the LC is less than 3m in 98% of the cases.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chghaf2023_ras</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{A Multimodal Loop Closure Fusion for Autonomous Vehicles SLAM}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chghaf, Mohammed and Rodríguez Flórez, Sergio Alberto and El Ouardi, Abdelhafid}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://www.sciencedirect.com/science/article/pii/S0921889023000854}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{Robotics and Autonomous Systems}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{Elsevier}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{165}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{104446}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{0921-8890}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1016/j.robot.2023.104446}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{SLAM, Camera, LiDAR, Localization, Loop closure, Mapping, Fusion, Multimodal}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{false}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">SPIE+OO</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/spie_2023-480.webp 480w,/assets/img/publication_preview/spie_2023-800.webp 800w,/assets/img/publication_preview/spie_2023-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/spie_2023.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="spie_2023.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="chghaf2023_spie" class="col-sm-8"> <div class="title">Real-time large-scale place recognition for autonomous ground vehicles using a spatial descriptor</div> <div class="author"> Mohammed Chghaf, Sergio Alberto Rodriguez Florez, and Abdelhafid El Ouardi </div> <div class="periodical"> <em>In SPIE Optics and Optoelectronics</em>, Jan 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://spie.org/optics-optoelectronics/presentation/Real-time-large-scale-place-recognition-for-autonomous-ground-vehicles/12571-2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://spie.org/optics-optoelectronics/presentation/Real-time-large-scale-place-recognition-for-autonomous-ground-vehicles/12571-2" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="abstract hidden"> <p>Place recognition is a key task in an autonomous vehicle’s Simultaneous Localization and Mapping (SLAM). The motion estimation is bound to drift over time due to cumulative errors. Fortunately, the correct identification of a revisited area provided by the place recognition module enables further optimizations that correct drifting errors if detected in real-time. Place recognition based on structural information of the scene is more robust to luminosity changes that can lead to false detections in the case of feature-based descriptors. However, they were mainly investigated in the context of depth sensors. Inspired by a LiDAR-based descriptor, we present a global geometric descriptor based on the structural information captured by a stereo camera. Using this descriptor, we can achieve real-time place recognition by focusing on the structural appearance of the scene derived from a 3D vision system. First, we introduce the approach used to record the 3D structural information of the visible space based on stereo images. Then, we conduct a parametric optimization protocol for precise place recognition in a given environment. Our experiments on the KITTI dataset show that the proposed approach is comparable to state-of-the-art methods, all while being low-cost. We studied the algorithm’s complexity to propose an optimized parallelization on GPU and SoC architectures. Performance evaluation on different hardware (GeForce RTX 3080, Jetson AGX Xavier, and Arria 10 SoC) shows that the real-time requirements of an embedded system are met. Compared to a CPU implementation, processing times showed a speed-up between 7x and 30x, depending on the architecture.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">chghaf2023_spie</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Real-time large-scale place recognition for autonomous ground vehicles using a spatial descriptor}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chghaf, Mohammed and Florez, Sergio Alberto Rodriguez and El Ouardi, Abdelhafid}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{SPIE Optics and Optoelectronics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100">JINT</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/survey_2022-480.webp 480w,/assets/img/publication_preview/survey_2022-800.webp 800w,/assets/img/publication_preview/survey_2022-1400.webp 1400w," type="image/webp" sizes="200px"> <img src="/assets/img/publication_preview/survey_2022.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="survey_2022.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="chghaf:hal-03647771" class="col-sm-8"> <div class="title">Camera, LiDAR and Multi-modal SLAM Systems for Autonomous Ground Vehicles: a Survey</div> <div class="author"> Mohammed Chghaf, Sergio Alberto Rodríguez Flórez, and Abdelhafid El Ouardi </div> <div class="periodical"> <em>Journal of Intelligent and Robotic Systems</em>, Apr 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1007/s10846-022-01582-8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://rdcu.be/cLMWA" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://link.springer.com/article/10.1007/s10846-022-01582-8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="badges"> <span class="__dimensions_badge_embed__" data-doi="10.1007/s10846-022-01582-8" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span> </div> <div class="abstract hidden"> <p>Simultaneous Localization and Mapping (SLAM) have been widely studied over the last years for autonomous vehicles. SLAM achieves its purpose by constructing a map of the unknown environment while keeping track of the location. A major challenge, which is paramount during the design of SLAM systems, lies in the efficient use of onboard sensors to perceive the environment. The most widely applied algorithms are camera-based SLAM and LiDAR-based SLAM. Recent research focuses on the fusion of camera-based and LiDAR-based frameworks that show promising results. In this paper, we present a study of commonly used sensors and the fundamental theories behind SLAM algorithms. The study then presents the hardware architectures used to process these algorithms and the performance obtained when possible. Secondly, we highlight state-of-the-art methodologies in each modality and in the multi-modal framework. A brief comparison followed by future challenges is then underlined. Additionally, we provide insights to possible fusion approaches that can increase the robustness and accuracy of modern SLAM algorithms; hence allowing the hardware-software co-design of embedded systems taking into account the algorithmic complexity and the embedded architectures and real-time constraints.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">chghaf:hal-03647771</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Camera, LiDAR and Multi-modal SLAM Systems for Autonomous Ground Vehicles: a Survey}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Chghaf, Mohammed and Rodríguez Flórez, Sergio Alberto and El Ouardi, Abdelhafid}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://link.springer.com/article/10.1007/s10846-022-01582-8?noAccess=true}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{{Journal of Intelligent and Robotic Systems}}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{Springer Verlag}}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{105}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{1}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">apr</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1007/s10846-022-01582-8}</span><span class="p">,</span>
  <span class="na">hal_id</span> <span class="p">=</span> <span class="s">{hal-03647769}</span><span class="p">,</span>
  <span class="na">hal_version</span> <span class="p">=</span> <span class="s">{v1}</span><span class="p">,</span>
  <span class="na">dimensions</span> <span class="p">=</span> <span class="s">{true}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6D%6F%68%61%6D%6D%65%64.%63%68%67%68%61%66@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/MohammedCHGHAF" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/mohammedchghaf" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://orcid.org/0000-0002-8725-1544" title="ORCID" rel="external nofollow noopener" target="_blank"><i class="ai ai-orcid"></i></a> <a href="https://www.researchgate.net/profile/Mohammed-Chghaf-2/" title="ResearchGate" rel="external nofollow noopener" target="_blank"><i class="ai ai-researchgate"></i></a> <a href="https://scholar.google.com/citations?user=rSdm4WkAAAAJ&amp;hl" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://mohammedchghaf.github.io/" title="Personal website"> <img src="http://mohammedchghaf.github.io/assets/img/profile.png" alt="Personal website"> </a> </div> <div class="contact-note">You can even add a little note about which of these is the best way to reach you. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Mohammed CHGHAF. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>